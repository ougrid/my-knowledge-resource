{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ougrid/my-knowledge-resource/blob/master/finetuning_xlm_withprompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "LjbEq0CpF0f8"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x4s5SJqF0f_"
      },
      "source": [
        "# Plan & Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:35:07.677742Z",
          "iopub.status.busy": "2024-05-09T15:35:07.677348Z",
          "iopub.status.idle": "2024-05-09T15:35:07.682866Z",
          "shell.execute_reply": "2024-05-09T15:35:07.681743Z",
          "shell.execute_reply.started": "2024-05-09T15:35:07.677707Z"
        },
        "trusted": true,
        "id": "9SQrOFJqF0gA"
      },
      "outputs": [],
      "source": [
        "#ปรับpromt\n",
        "#tagชื่อ\n",
        "#translate\n",
        "#มองดูว่าคำถามตรงไหนmatchกัน\n",
        "#ใช้pattern แยกคำถามด้วยrule baseก่อน"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8oj6hA1F0gB"
      },
      "source": [
        "#Import Librery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:35:08.229382Z",
          "iopub.status.busy": "2024-05-09T15:35:08.228655Z",
          "iopub.status.idle": "2024-05-09T15:35:15.565337Z",
          "shell.execute_reply": "2024-05-09T15:35:15.564519Z",
          "shell.execute_reply.started": "2024-05-09T15:35:08.229350Z"
        },
        "trusted": true,
        "id": "LbrKs8_HF0gB",
        "outputId": "5c3f04da-9e96-460b-a94f-3d69c3c97774"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-09 15:35:13.108344: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-09 15:35:13.108400: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-09 15:35:13.109895: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "#import basic librery\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split #how to split data\n",
        "\n",
        "#basic torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd         # computation graph\n",
        "from torch import Tensor                  # tensor node in the computation graph\n",
        "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
        "\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import ast\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from torch.jit import script, trace       # hybrid frontend decorator and tracing jit\n",
        "from sklearn.metrics import f1_score\n",
        "from datetime import datetime\n",
        "import random\n",
        "import time\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup, TFXLMRobertaModel,  XLMRobertaForSequenceClassification\n",
        "\n",
        "#classic tool & classic ML\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:35:15.567782Z",
          "iopub.status.busy": "2024-05-09T15:35:15.567228Z",
          "iopub.status.idle": "2024-05-09T15:35:15.599055Z",
          "shell.execute_reply": "2024-05-09T15:35:15.598087Z",
          "shell.execute_reply.started": "2024-05-09T15:35:15.567752Z"
        },
        "trusted": true,
        "id": "T9rgK68BF0gC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:35:15.602020Z",
          "iopub.status.busy": "2024-05-09T15:35:15.601328Z",
          "iopub.status.idle": "2024-05-09T15:35:15.605946Z",
          "shell.execute_reply": "2024-05-09T15:35:15.605022Z",
          "shell.execute_reply.started": "2024-05-09T15:35:15.601989Z"
        },
        "trusted": true,
        "id": "Z0ut0cT_F0gC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:35:15.609729Z",
          "iopub.status.busy": "2024-05-09T15:35:15.608610Z",
          "iopub.status.idle": "2024-05-09T15:35:28.454529Z",
          "shell.execute_reply": "2024-05-09T15:35:28.453302Z",
          "shell.execute_reply.started": "2024-05-09T15:35:15.609699Z"
        },
        "trusted": true,
        "id": "X3mI9XDBF0gD",
        "outputId": "f1d835b9-fab2-4c97-8cce-5a355c5e8c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:35:28.456331Z",
          "iopub.status.busy": "2024-05-09T15:35:28.456000Z",
          "iopub.status.idle": "2024-05-09T15:35:41.457359Z",
          "shell.execute_reply": "2024-05-09T15:35:41.456362Z",
          "shell.execute_reply.started": "2024-05-09T15:35:28.456299Z"
        },
        "trusted": true,
        "id": "VjYUkk6oF0gD",
        "outputId": "65dfa2a4-e133-4b8f-d533-c947347b4978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[tf-cpu] in /opt/conda/lib/python3.10/site-packages (4.39.3)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (4.66.1)\n",
            "Requirement already satisfied: tensorflow-cpu<2.16,>=2.6 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (2.15.1)\n",
            "Requirement already satisfied: onnxconverter-common in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (1.13.0)\n",
            "Requirement already satisfied: tf2onnx in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (1.16.1)\n",
            "Requirement already satisfied: tensorflow-text<2.16 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (2.15.0)\n",
            "Requirement already satisfied: keras-nlp>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[tf-cpu]) (0.9.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[tf-cpu]) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[tf-cpu]) (4.9.0)\n",
            "Requirement already satisfied: keras-core in /opt/conda/lib/python3.10/site-packages (from keras-nlp>=0.3.1->transformers[tf-cpu]) (0.1.7)\n",
            "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-nlp>=0.3.1->transformers[tf-cpu]) (1.4.0)\n",
            "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-nlp>=0.3.1->transformers[tf-cpu]) (13.7.0)\n",
            "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-nlp>=0.3.1->transformers[tf-cpu]) (0.1.8)\n",
            "Requirement already satisfied: kagglehub in /opt/conda/lib/python3.10/site-packages (from keras-nlp>=0.3.1->transformers[tf-cpu]) (0.2.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[tf-cpu]) (3.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (69.0.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (1.51.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text<2.16->transformers[tf-cpu]) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text<2.16->transformers[tf-cpu]) (2.15.1)\n",
            "Requirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (from onnxconverter-common->transformers[tf-cpu]) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[tf-cpu]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[tf-cpu]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[tf-cpu]) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[tf-cpu]) (2024.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (2.26.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (3.0.2)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.13.0->tensorflow-text<2.16->transformers[tf-cpu]) (2.15.1)\n",
            "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras-core->keras-nlp>=0.3.1->transformers[tf-cpu]) (0.0.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-nlp>=0.3.1->transformers[tf-cpu]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-nlp>=0.3.1->transformers[tf-cpu]) (2.17.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp>=0.3.1->transformers[tf-cpu]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-cpu<2.16,>=2.6->transformers[tf-cpu]) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[tf-cpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:09.643650Z",
          "iopub.status.busy": "2024-05-09T15:36:09.643253Z",
          "iopub.status.idle": "2024-05-09T15:36:22.023368Z",
          "shell.execute_reply": "2024-05-09T15:36:22.021881Z",
          "shell.execute_reply.started": "2024-05-09T15:36:09.643616Z"
        },
        "trusted": true,
        "id": "y5eH7BraF0gE"
      },
      "outputs": [],
      "source": [
        "!pip install pythainlp -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:22.025999Z",
          "iopub.status.busy": "2024-05-09T15:36:22.025610Z",
          "iopub.status.idle": "2024-05-09T15:36:22.031271Z",
          "shell.execute_reply": "2024-05-09T15:36:22.030141Z",
          "shell.execute_reply.started": "2024-05-09T15:36:22.025965Z"
        },
        "trusted": true,
        "id": "qaPeWapyF0gE"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:22.033107Z",
          "iopub.status.busy": "2024-05-09T15:36:22.032709Z",
          "iopub.status.idle": "2024-05-09T15:36:22.087376Z",
          "shell.execute_reply": "2024-05-09T15:36:22.086622Z",
          "shell.execute_reply.started": "2024-05-09T15:36:22.033070Z"
        },
        "trusted": true,
        "id": "ppJ4g1AcF0gE"
      },
      "outputs": [],
      "source": [
        "#Import data set\n",
        "train_df = pd.read_csv('/kaggle/input/super-train-title/super_train_title.csv', index_col = 0)\n",
        "#test_df = pd.read_csv('test_add_tag.csv', index_col = 0)\n",
        "# pat_df = pd.read_csv('/kaggle/input/legal-act-classification/patterns.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:22.089845Z",
          "iopub.status.busy": "2024-05-09T15:36:22.089503Z",
          "iopub.status.idle": "2024-05-09T15:36:22.409357Z",
          "shell.execute_reply": "2024-05-09T15:36:22.408534Z",
          "shell.execute_reply.started": "2024-05-09T15:36:22.089818Z"
        },
        "trusted": true,
        "id": "elZqbETVF0gF"
      },
      "outputs": [],
      "source": [
        "#create prompt\n",
        "\n",
        "prompts = []\n",
        "for index, row in train_df.iterrows():\n",
        "    prompt = f'''\n",
        "    คุณเป็นผู้ช่วยทางด้านนิติกรรม คุณจะต้องตอบคำถามทางด้านนิติกรรมโดยใช้ข้อมูลต้นฉบับเท่านั้น\\n\n",
        "    ข้อมูลต้นฉบับ : {row['context']} \\n\n",
        "    คำถาม : คุณ {row['question']} จะทำการ {row['legal_act']} ทำได้หรือไม่? ตอบแค่ได้ หรือ ไม่ได้ ถ้าได้ให้ตอบ 1 และถ้าไม่ได้ให้ตอบ 0'''\n",
        "    prompts.append(prompt)\n",
        "\n",
        "# Create a new column named 'prompt' with the generated prompts\n",
        "train_df['prompt'] = prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:50:51.848838Z",
          "iopub.status.busy": "2024-05-09T15:50:51.848312Z",
          "iopub.status.idle": "2024-05-09T15:50:52.021873Z",
          "shell.execute_reply": "2024-05-09T15:50:52.020863Z",
          "shell.execute_reply.started": "2024-05-09T15:50:51.848799Z"
        },
        "trusted": true,
        "id": "Dgxdn9D0F0gF"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv(\"/kaggle/working/fortranslate.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:46:55.909444Z",
          "iopub.status.busy": "2024-05-09T15:46:55.908598Z",
          "iopub.status.idle": "2024-05-09T15:46:55.915928Z",
          "shell.execute_reply": "2024-05-09T15:46:55.914890Z",
          "shell.execute_reply.started": "2024-05-09T15:46:55.909407Z"
        },
        "trusted": true,
        "id": "3UGQTmIyF0gF",
        "outputId": "2fb2dd53-c051-4381-f8fd-62add781c693"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4429"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:22.418501Z",
          "iopub.status.busy": "2024-05-09T15:36:22.418213Z",
          "iopub.status.idle": "2024-05-09T15:36:22.427935Z",
          "shell.execute_reply": "2024-05-09T15:36:22.426841Z",
          "shell.execute_reply.started": "2024-05-09T15:36:22.418476Z"
        },
        "scrolled": true,
        "trusted": true,
        "id": "hi80-bXBF0gF",
        "outputId": "4ce7eb15-d6f6-41f6-f740-a60844250271"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['id', 'rgno', 'context', 'pattern', 'question', 'legal_act',\n",
              "       'condition', 'answer', 'prompt'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:22.429597Z",
          "iopub.status.busy": "2024-05-09T15:36:22.429227Z",
          "iopub.status.idle": "2024-05-09T15:36:22.435256Z",
          "shell.execute_reply": "2024-05-09T15:36:22.434416Z",
          "shell.execute_reply.started": "2024-05-09T15:36:22.429550Z"
        },
        "trusted": true,
        "id": "pK2Nd6LTF0gG"
      },
      "outputs": [],
      "source": [
        "#train_df = train_df.drop(columns=['prompt'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:22.436934Z",
          "iopub.status.busy": "2024-05-09T15:36:22.436561Z",
          "iopub.status.idle": "2024-05-09T15:36:22.446760Z",
          "shell.execute_reply": "2024-05-09T15:36:22.445976Z",
          "shell.execute_reply.started": "2024-05-09T15:36:22.436900Z"
        },
        "trusted": true,
        "id": "a9L0DFz_F0gG"
      },
      "outputs": [],
      "source": [
        "#data split\n",
        "\n",
        "X = train_df['prompt']  # Your features\n",
        "y = train_df['answer']  # Your target labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#convert into PyTorch Datasets\n",
        "#train_data = TensorDataset(X_train,y_train)\n",
        "\n",
        "#translate into dataloader objects\n",
        "#batchsize    = 8\n",
        "#train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "\n",
        "#print(f\"Training set size: {len(train_data)}\")\n",
        "\n",
        "#print(f\"Validation set shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:22.448189Z",
          "iopub.status.busy": "2024-05-09T15:36:22.447868Z",
          "iopub.status.idle": "2024-05-09T15:36:25.484208Z",
          "shell.execute_reply": "2024-05-09T15:36:25.483370Z",
          "shell.execute_reply.started": "2024-05-09T15:36:22.448164Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "b6HDGHUrF0gG",
        "outputId": "4c0edbc1-3f9d-453c-cf1c-3a61760b5d88"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#load model\n",
        "models = [\n",
        "    AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
        "        #num_labels=2,  # Assuming binary classification\n",
        "        #hidden_dropout_prob=0.2\n",
        "    ),\n",
        "    #AutoModelForSequenceClassification.from_pretrained(\n",
        "    AutoModelForSequenceClassification.from_pretrained(\n",
        "        'FacebookAI/xlm-roberta-base'#,\n",
        "        #hidden_size=512,  # Example: customize hidden size\n",
        "        #num_attention_heads=8\n",
        "    ),]\n",
        "\n",
        "#choose tokenizer\n",
        "tokenizers = [\n",
        "    AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\"),\n",
        "    AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base',  problem_type=\"multi_label_classification\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:25.487974Z",
          "iopub.status.busy": "2024-05-09T15:36:25.487254Z",
          "iopub.status.idle": "2024-05-09T15:36:27.625221Z",
          "shell.execute_reply": "2024-05-09T15:36:27.624355Z",
          "shell.execute_reply.started": "2024-05-09T15:36:25.487935Z"
        },
        "trusted": true,
        "id": "Nz3_eC5eF0gG",
        "outputId": "18c4995d-5cf5-43fb-ba95-8573eb32684b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model =  AutoModelForSequenceClassification.from_pretrained('FacebookAI/xlm-roberta-base')\n",
        "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base',  problem_type=\"multi_label_classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "7--DJ0gAF0gG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:31.569976Z",
          "iopub.status.busy": "2024-05-09T15:36:31.569218Z",
          "iopub.status.idle": "2024-05-09T15:36:31.577935Z",
          "shell.execute_reply": "2024-05-09T15:36:31.576920Z",
          "shell.execute_reply.started": "2024-05-09T15:36:31.569941Z"
        },
        "trusted": true,
        "id": "TOX6LmshF0gG"
      },
      "outputs": [],
      "source": [
        "# df_train.review = df_train.str.lower()\n",
        "sentences = train_df.prompt.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = train_df.answer.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:33.057608Z",
          "iopub.status.busy": "2024-05-09T15:36:33.057216Z",
          "iopub.status.idle": "2024-05-09T15:36:34.951650Z",
          "shell.execute_reply": "2024-05-09T15:36:34.950735Z",
          "shell.execute_reply.started": "2024-05-09T15:36:33.057558Z"
        },
        "trusted": true,
        "id": "ihI92aovF0gG",
        "outputId": "e22cb542-c679-433d-fe9b-ec77fb7d1b57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (761 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['▁[', 'C', 'LS', ']', '▁คุณ', 'เป็น', 'ผู้ช่วย', 'ทางด้าน', 'นิติ', 'กรรม', '▁คุณ', 'จะต้อง', 'ตอบ', 'คําถาม', 'ทางด้าน', 'นิติ', 'กรรม', 'โดยใช้', 'ข้อมูล', 'ต้น', 'ฉบับ', 'เท่านั้น', '▁ข้อมูล', 'ต้น', 'ฉบับ', '▁:', '▁', 'กรรม', 'การ', 'คน', 'ใด', 'คนหนึ่ง', 'ลง', 'ลาย', 'มือ', 'ชื่อ', 'ร่วมกับ', 'กรรม', 'การ', 'อื่น', 'อีกหนึ่ง', 'คน', 'รวม', 'เป็น', 'สอง', 'คน', 'และ', 'ประ', 'ทับ', 'ตรา', 'สําคัญ', 'ของ', 'บริษัท', '▁', 'คําถาม', '▁:', '▁คุณ', '▁[', \"'\", 'P', '000', '6', \"'\", ']', '▁จะ', 'ทําการ', '▁', 'การทํา', 'นิติ', 'กรรม', '▁สํานักงาน', 'ตรวจ', 'คน', 'เข้า', 'เมือง', '▁', 'ทําได้', 'หรือไม่', '?', '▁', 'ตอบ', 'แค่', 'ได้', '▁หรือ', '▁', 'ไม่ได้', '▁ถ้า', 'ได้', 'ให้', 'ตอบ', '▁1', '▁และ', 'ถ้า', 'ไม่ได้', 'ให้', 'ตอบ', '▁0', '▁[', 'S', 'EP', ']']\n"
          ]
        }
      ],
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "RG3kswI2F0gH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:35.217936Z",
          "iopub.status.busy": "2024-05-09T15:36:35.217293Z",
          "iopub.status.idle": "2024-05-09T15:36:35.733477Z",
          "shell.execute_reply": "2024-05-09T15:36:35.732494Z",
          "shell.execute_reply.started": "2024-05-09T15:36:35.217904Z"
        },
        "trusted": true,
        "id": "GSVoLodyF0gH",
        "outputId": "84233898-d5c4-4d8c-a440-8d10f0f9ed46",
        "colab": {
          "referenced_widgets": [
            "953f6251c5a1490b8fe89d8fc6f065d5"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_94/1317061965.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for i in tqdm_notebook(range(len(tokenized_texts))):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "953f6251c5a1490b8fe89d8fc6f065d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4429 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#give index number for tokenized word\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm import  tqdm_notebook\n",
        "#uncomment all three lines below to convert to ids\n",
        "input_ids=[]\n",
        "for i in tqdm_notebook(range(len(tokenized_texts))):\n",
        "  input_ids.append(tokenizer.convert_tokens_to_ids(tokenized_texts[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "VSQXRGnVF0gH"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "l0SVRxy3F0gH"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:39.420799Z",
          "iopub.status.busy": "2024-05-09T15:36:39.420166Z",
          "iopub.status.idle": "2024-05-09T15:36:39.501395Z",
          "shell.execute_reply": "2024-05-09T15:36:39.500638Z",
          "shell.execute_reply.started": "2024-05-09T15:36:39.420767Z"
        },
        "trusted": true,
        "id": "pR7TenDoF0gH"
      },
      "outputs": [],
      "source": [
        "#standardize code\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 256\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:41.450882Z",
          "iopub.status.busy": "2024-05-09T15:36:41.450168Z",
          "iopub.status.idle": "2024-05-09T15:36:42.219706Z",
          "shell.execute_reply": "2024-05-09T15:36:42.218874Z",
          "shell.execute_reply.started": "2024-05-09T15:36:41.450849Z"
        },
        "trusted": true,
        "id": "AzQcMs8iF0gH"
      },
      "outputs": [],
      "source": [
        "#Create attention masks\n",
        "attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "#Machine learning models shouldn't waste effort processing padding tokens.  The attention mask lets the model know which tokens to focus on and which to ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:43.934774Z",
          "iopub.status.busy": "2024-05-09T15:36:43.934378Z",
          "iopub.status.idle": "2024-05-09T15:36:43.947708Z",
          "shell.execute_reply": "2024-05-09T15:36:43.946653Z",
          "shell.execute_reply.started": "2024-05-09T15:36:43.934740Z"
        },
        "trusted": true,
        "id": "vAOKTuQAF0gH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,random_state=56, test_size=0.2)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=56, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:45.754189Z",
          "iopub.status.busy": "2024-05-09T15:36:45.753787Z",
          "iopub.status.idle": "2024-05-09T15:36:46.090855Z",
          "shell.execute_reply": "2024-05-09T15:36:46.090015Z",
          "shell.execute_reply.started": "2024-05-09T15:36:45.754147Z"
        },
        "trusted": true,
        "id": "UoEiEUxAF0gH"
      },
      "outputs": [],
      "source": [
        "#Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:47.797702Z",
          "iopub.status.busy": "2024-05-09T15:36:47.797298Z",
          "iopub.status.idle": "2024-05-09T15:36:47.804526Z",
          "shell.execute_reply": "2024-05-09T15:36:47.803585Z",
          "shell.execute_reply.started": "2024-05-09T15:36:47.797668Z"
        },
        "trusted": true,
        "id": "Q4AjlcqTF0gH"
      },
      "outputs": [],
      "source": [
        "#data to loader\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:49.988290Z",
          "iopub.status.busy": "2024-05-09T15:36:49.987257Z",
          "iopub.status.idle": "2024-05-09T15:36:50.049624Z",
          "shell.execute_reply": "2024-05-09T15:36:50.048591Z",
          "shell.execute_reply.started": "2024-05-09T15:36:49.988252Z"
        },
        "trusted": true,
        "id": "VhmMkLVaF0gI"
      },
      "outputs": [],
      "source": [
        "## Define model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:36:51.641922Z",
          "iopub.status.busy": "2024-05-09T15:36:51.641280Z",
          "iopub.status.idle": "2024-05-09T15:37:10.819948Z",
          "shell.execute_reply": "2024-05-09T15:37:10.819102Z",
          "shell.execute_reply.started": "2024-05-09T15:36:51.641887Z"
        },
        "trusted": true,
        "id": "dlRdZkBbF0gI",
        "outputId": "f7ff71fe-0a49-405d-885b-50622a2f0a6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch_transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_transformers) (2.1.2)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch_transformers) (1.26.4)\n",
            "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from pytorch_transformers) (1.26.100)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pytorch_transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch_transformers) (4.66.1)\n",
            "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from pytorch_transformers) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from pytorch_transformers) (0.2.0)\n",
            "Collecting sacremoses (from pytorch_transformers)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_transformers) (4.9.0)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_transformers) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->pytorch_transformers) (2024.2.0)\n",
            "Collecting botocore<1.30.0,>=1.29.100 (from boto3->pytorch_transformers)\n",
            "  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch_transformers) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch_transformers) (0.6.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch_transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch_transformers) (2024.2.2)\n",
            "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses->pytorch_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses->pytorch_transformers) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3->pytorch_transformers) (2.9.0.post0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->pytorch_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->pytorch_transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3->pytorch_transformers) (1.16.0)\n",
            "Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, botocore, pytorch_transformers\n",
            "  Attempting uninstall: botocore\n",
            "    Found existing installation: botocore 1.34.69\n",
            "    Uninstalling botocore-1.34.69:\n",
            "      Successfully uninstalled botocore-1.34.69\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.29.165 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed botocore-1.29.165 pytorch_transformers-1.2.0 sacremoses-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_transformers\n",
        "from pytorch_transformers import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:37:10.821967Z",
          "iopub.status.busy": "2024-05-09T15:37:10.821654Z",
          "iopub.status.idle": "2024-05-09T15:37:24.053009Z",
          "shell.execute_reply": "2024-05-09T15:37:24.051845Z",
          "shell.execute_reply.started": "2024-05-09T15:37:10.821941Z"
        },
        "trusted": true,
        "id": "HBBevmzmF0gI",
        "outputId": "91cdd6ff-69ae-4a51-bac9-d9426231f945"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m943.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from torch_optimizer) (2.1.2)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (4.9.0)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (2024.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.5.0->torch_optimizer) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
            "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch_optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:37:24.054919Z",
          "iopub.status.busy": "2024-05-09T15:37:24.054543Z",
          "iopub.status.idle": "2024-05-09T15:37:24.074388Z",
          "shell.execute_reply": "2024-05-09T15:37:24.073636Z",
          "shell.execute_reply.started": "2024-05-09T15:37:24.054887Z"
        },
        "trusted": true,
        "id": "kblr-bLAF0gI"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch_optimizer import RAdam # Install via: pip install torch_optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:37:24.076910Z",
          "iopub.status.busy": "2024-05-09T15:37:24.076465Z",
          "iopub.status.idle": "2024-05-09T15:37:24.084456Z",
          "shell.execute_reply": "2024-05-09T15:37:24.083479Z",
          "shell.execute_reply.started": "2024-05-09T15:37:24.076881Z"
        },
        "trusted": true,
        "id": "XpYvLawPF0gI"
      },
      "outputs": [],
      "source": [
        "#meta parameter\n",
        "\n",
        "lr = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "num_total_steps = 1000\n",
        "num_warmup_steps = 100\n",
        "warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
        "\n",
        "\n",
        "### In PyTorch-Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "optimizer = RAdam(model.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:37:24.086436Z",
          "iopub.status.busy": "2024-05-09T15:37:24.086051Z",
          "iopub.status.idle": "2024-05-09T15:37:24.097382Z",
          "shell.execute_reply": "2024-05-09T15:37:24.096389Z",
          "shell.execute_reply.started": "2024-05-09T15:37:24.086396Z"
        },
        "trusted": true,
        "id": "rWP2K7tEF0gI",
        "outputId": "3ceebace-b75d-473e-81b2-3687eeaa893c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "222"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:39:03.264985Z",
          "iopub.status.busy": "2024-05-09T15:39:03.264281Z",
          "iopub.status.idle": "2024-05-09T15:39:03.579545Z",
          "shell.execute_reply": "2024-05-09T15:39:03.578557Z",
          "shell.execute_reply.started": "2024-05-09T15:39:03.264948Z"
        },
        "trusted": true,
        "id": "ik67XZdlF0gI",
        "outputId": "46806c18-876e-409a-b370-e0442ae6fb56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "XLMRobertaForSequenceClassification(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): XLMRobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:39:06.442352Z",
          "iopub.status.busy": "2024-05-09T15:39:06.441935Z",
          "iopub.status.idle": "2024-05-09T15:44:40.354704Z",
          "shell.execute_reply": "2024-05-09T15:44:40.353753Z",
          "shell.execute_reply.started": "2024-05-09T15:39:06.442315Z"
        },
        "trusted": true,
        "id": "ODsPYzNbF0gN",
        "outputId": "3ab45dad-9f58-43ae-f573-993ca2c39dc7",
        "colab": {
          "referenced_widgets": [
            "335d966455554e45bfe8725c08996a36"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_94/2779366120.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for epoch in tqdm_notebook(range(epochs)):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "335d966455554e45bfe8725c08996a36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [1/222], Loss: 0.7526\n",
            "Epoch [1/2], Step [51/222], Loss: 0.6981\n",
            "Epoch [1/2], Step [101/222], Loss: 0.5608\n",
            "Epoch [1/2], Step [151/222], Loss: 0.6486\n",
            "Epoch [1/2], Step [201/222], Loss: 0.5211\n",
            "Epoch [2/2], Step [1/222], Loss: 0.6499\n",
            "Epoch [2/2], Step [51/222], Loss: 0.6817\n",
            "Epoch [2/2], Step [101/222], Loss: 0.4643\n",
            "Epoch [2/2], Step [151/222], Loss: 0.4242\n",
            "Epoch [2/2], Step [201/222], Loss: 0.2265\n"
          ]
        }
      ],
      "source": [
        "total_step = len(train_dataloader)\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for epoch in tqdm_notebook(range(epochs)):\n",
        "\n",
        "\n",
        "\n",
        "    # Training\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Forward pass\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "      loss = outputs[0]\n",
        "      train_loss_set.append(loss.item())\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      # Update parameters and take a step using the computed gradient\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      if (i) % 50 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-09T15:55:25.985850Z",
          "iopub.status.busy": "2024-05-09T15:55:25.984958Z",
          "iopub.status.idle": "2024-05-09T15:55:28.025971Z",
          "shell.execute_reply": "2024-05-09T15:55:28.024885Z",
          "shell.execute_reply.started": "2024-05-09T15:55:25.985808Z"
        },
        "trusted": true,
        "id": "34OS7gunF0gN"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"kuymodel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "IqLRooQmF0gN"
      },
      "outputs": [],
      "source": [
        "total_step = len(train_dataloader)\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for epoch in tqdm_notebook(range(epochs)):\n",
        "\n",
        "\n",
        "\n",
        "    # Training\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Forward pass\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "      loss = outputs[0]\n",
        "      train_loss_set.append(loss.item())\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      # Update parameters and take a step using the computed gradient\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      if (i) % 50 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8-qtBmlF0gN"
      },
      "source": [
        "#ไม่ได้ใช้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "YJV4nLdzF0gN"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "svm_model = SVC(kernel='linear')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "Ysk0L5WLF0gO"
      },
      "outputs": [],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "9e7hQ3UyF0gO"
      },
      "outputs": [],
      "source": [
        "# Feature Extraction (for SVM)\n",
        "X_train_svm = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_svm = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train SVM\n",
        "svm_model.fit(X_train_svm, y_train)\n",
        "\n",
        "# Evaluate SVM\n",
        "svm_predictions = svm_model.predict(X_test_svm)\n",
        "svm_f1_macro = f1_score(y_test, svm_predictions, average='macro')  # F1-score (macro average)\n",
        "print(f\"SVM F1-Macro: {svm_f1_macro}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "8fe7fcSsF0gO"
      },
      "outputs": [],
      "source": [
        "print(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "Es3UaohVF0gO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFXLMRobertaForMaskedLM\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the XLM-RoBERTa model\n",
        "model = models[1]\n",
        "tokenizer = tokenizers[1]  # Assuming you want to use the tokenizer corresponding to the XLM-RoBERTa model\n",
        "\n",
        "# Tokenize the input text\n",
        "text = \"The capital of France is <mask>.\"\n",
        "inputs = tokenizer(text, return_tensors=\"tf\")\n",
        "print(type(inputs))\n",
        "#print(tokenizer)\n",
        "#print(inputs[0])\n",
        "\n",
        "# Get the logits from the XLM-RoBERTa model\n",
        "logits = model(inputs)[0]  # Output of the XLM-RoBERTa model\n",
        "\n",
        "# Retrieve the index of the masked token\n",
        "mask_token_index = tf.where(tf.equal(inputs[\"input_ids\"], tokenizer.mask_token_id))\n",
        "\n",
        "# Extract the logits corresponding to the masked token\n",
        "selected_logits = tf.gather_nd(logits, mask_token_index)\n",
        "\n",
        "# Get the predicted token ID\n",
        "predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n",
        "\n",
        "# Decode the predicted token ID\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "print(predicted_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "pRzl_-kHF0gO"
      },
      "outputs": [],
      "source": [
        "type(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "cL9Bg62vF0gO"
      },
      "outputs": [],
      "source": [
        "# Ensemble Prediction Function\n",
        "def predict_ensemble(models, tokenizers, svm_model, X, for_svm=False):\n",
        "  predictions = []\n",
        "\n",
        "  for text in X:\n",
        "    transformer_preds = []\n",
        "    for model, tokenizer in zip(models, tokenizers):\n",
        "      inputs = tokenizer(text, return_tensors=\"tf\")\n",
        "      print(inputs)\n",
        "\n",
        "      # Assuming your models have a .predict() method for raw predictions\n",
        "      model_preds = model(inputs).argmax(axis=-1)[0]\n",
        "      transformer_preds.append(model_preds)\n",
        "\n",
        "    svm_input = preprocess_text(text, None, for_svm=True)\n",
        "    svm_pred = svm_model.predict(svm_input)[0]  # Get class prediction\n",
        "\n",
        "    # Implement your ensemble logic (e.g., majority voting)\n",
        "    if sum(transformer_preds) > len(transformer_preds) / 2 and svm_pred == 1:\n",
        "      ensemble_prediction = 1\n",
        "    else:\n",
        "      ensemble_prediction = 0\n",
        "\n",
        "    predictions.append(ensemble_prediction)\n",
        "\n",
        "  return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "VZw6AyO3F0gO"
      },
      "outputs": [],
      "source": [
        "# Prediction on Test Set\n",
        "ensemble_predictions = predict_ensemble(models, tokenizers, svm_model, X_test.tolist(), for_svm=True)\n",
        "\n",
        "# Evaluation using F1-score\n",
        "f1_macro = f1_score(y_test, ensemble_predictions, average='macro')\n",
        "print(f\"Ensemble F1-Macro: {f1_macro}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zrIVvcTF0gO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4969544,
          "sourceId": 8361705,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30698,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}