{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ougrid/my-knowledge-resource/blob/master/reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Reinforcement Learning "
      ],
      "metadata": {
        "id": "QZkyeTdex4JU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Value Iteration\n",
        "2. Q-Learning \n",
        "\n",
        "ref: https://gibberblot.github.io/rl-notes/single-agent/MDPs.html\n"
      ],
      "metadata": {
        "id": "-4UCz-uryF02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade gdown\n",
        "!gdown --folder --no-check-certificate https://drive.google.com/drive/folders/1Cg-nfqseH-vB4eCRUY3hc1sFJ8Xnrvjw?usp=share_link  \n",
        "!cp python_code/* ."
      ],
      "metadata": {
        "id": "5iLD8Jx4eEHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gridworld import *"
      ],
      "metadata": {
        "id": "hg3B5AF4j9ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gw = GridWorld()\n",
        "gw.visualise()"
      ],
      "metadata": {
        "id": "6OS5v6GkyJ3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things can go wrong â€” sometimes the effects of the actions are not what we want:\n",
        "\n",
        "* If the agent tries to move north, 80% of the time, this works as planned (provided the wall is not in the way)\n",
        "\n",
        "* 10 %\n",
        " of the time, trying to move north takes the agent west (provided the wall is not in the way);\n",
        "\n",
        "* 10 %\n",
        " of the time, trying to move north takes the agent east (provided the wall is not in the way)\n",
        "\n",
        "* If the wall is in the way of the cell that would have been taken, the agent stays in the current cell."
      ],
      "metadata": {
        "id": "2Ljl5jRgDd0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from value_function import ValueFunction\n",
        "from qtable import QFunction\n",
        "\n",
        "class TabularValueFunction(ValueFunction):\n",
        "    def __init__(self, default=0.0):\n",
        "        self.value_table = defaultdict(lambda: default)\n",
        "\n",
        "    def update(self, state, value):\n",
        "        self.value_table[state] = value\n",
        "\n",
        "    def merge(self, value_table):\n",
        "        for state in value_table.value_table.keys():\n",
        "            self.update(state, value_table.get_value(state))\n",
        "\n",
        "    def get_value(self, state):\n",
        "        return self.value_table[state]\n",
        "\n",
        "class QTable(QFunction):\n",
        "    def __init__(self, default=0.0):\n",
        "        self.qtable = defaultdict(lambda: default)\n",
        "\n",
        "    def update(self, state, action, qvalue):\n",
        "        self.qtable[(state, action)] = qvalue\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        return self.qtable[(state, action)]\n",
        "    \n",
        "    def merge(self, qtable):\n",
        "        for state, action in qtable.qtable.keys():\n",
        "            self.update(state, action, qtable.get_q_value(state, action))\n"
      ],
      "metadata": {
        "id": "8_V_HAHAx_VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = TabularValueFunction()\n",
        "qtable = QTable()"
      ],
      "metadata": {
        "id": "s1qeC9cljRjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with an initial guess for the value function"
      ],
      "metadata": {
        "id": "Q1A1a-u10apc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gridworld = GridWorld()\n",
        "gridworld.visualise_value_function(values, \"Value function\")"
      ],
      "metadata": {
        "id": "_-cLghvmv8Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gridworld.visualise_q_function(qtable, \"Q-Value function\")"
      ],
      "metadata": {
        "id": "EiVEQ3U9zZy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = values.extract_policy(gridworld)\n",
        "gridworld.visualise_policy(policy, \"Policy\")"
      ],
      "metadata": {
        "id": "CKBMbIpDwIfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q(s,a) = E[ r + gamma * V(s')]\n",
        "\n",
        "V(s')  = max Q(s', a')"
      ],
      "metadata": {
        "id": "JEv087SdzDZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = TabularValueFunction()\n",
        "qtable = QTable()"
      ],
      "metadata": {
        "id": "5t5wKeUF23qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gw.get_discount_factor()"
      ],
      "metadata": {
        "id": "ETASyao93XnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_values = TabularValueFunction()\n",
        "new_qtable = QTable()\n",
        "\n",
        "for state in gw.get_states():\n",
        "  for action in gw.get_actions(state):\n",
        "\n",
        "    new_value = 0.0\n",
        "    for (new_state, probability) in gw.get_transitions(state, action):\n",
        "\n",
        "      reward = gw.get_reward(state, action, new_state)\n",
        "      new_value += probability * ( reward + (gw.get_discount_factor() * values.get_value(new_state)))\n",
        "\n",
        "    new_qtable.update(state, action, new_value)\n",
        "\n",
        "  (_, max_q) = new_qtable.get_max_q(state, gw.get_actions(state))\n",
        "  new_values.update(state, max_q)\n",
        "\n",
        "values.merge(new_values)\n",
        "qtable.merge(new_qtable)"
      ],
      "metadata": {
        "id": "Yzie5NxWxAfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-step value propagate back from the rewarded state\n",
        "# try run the previous cell couple of times to see how the value changes\n",
        "gridworld.visualise_value_function(values, \"Value function\")"
      ],
      "metadata": {
        "id": "JR1kU0qNxhTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gridworld.visualise_q_function(qtable, \"Q function\")"
      ],
      "metadata": {
        "id": "sWmYyanB2MA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = TabularValueFunction()\n",
        "qtable = QTable()\n",
        "\n",
        "max_iterations = 1000\n",
        "threshold = 0.1\n",
        "for i in range(max_iterations):\n",
        "  new_values = TabularValueFunction()\n",
        "  new_qtable = QTable()\n",
        "  delta = 0.0 \n",
        "\n",
        "  for state in gw.get_states():\n",
        "    for action in gw.get_actions(state):\n",
        "\n",
        "      new_value = 0.0\n",
        "      for (new_state, probability) in gw.get_transitions(state, action):\n",
        "\n",
        "        reward = gw.get_reward(state, action, new_state)\n",
        "        new_value += probability * ( reward + (gw.get_discount_factor() * values.get_value(new_state)))\n",
        "\n",
        "      new_qtable.update(state, action, new_value)\n",
        "\n",
        "    (_, max_q) = new_qtable.get_max_q(state, gw.get_actions(state))\n",
        "    \n",
        "    delta = max(delta, abs(values.get_value(state) - max_q))\n",
        "    new_values.update(state, max_q)\n",
        "\n",
        "  values.merge(new_values)\n",
        "  qtable.merge(new_qtable)\n",
        "\n",
        "  if delta < threshold:\n",
        "    print(i)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "UAiQ47dO9NER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gridworld.visualise_value_function(values, \"Value function\")"
      ],
      "metadata": {
        "id": "V_-Gil7GA68i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gridworld.visualise_q_function(qtable, \"Q function\")"
      ],
      "metadata": {
        "id": "YMVQo1Z6BGei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aGOqgJQmBIIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement Learning is needed when transition probability and reward is unknown.\n",
        "We need to interact with the environment to gain knowledge of what does the environment look like."
      ],
      "metadata": {
        "id": "10VE538CChT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnknownWorld():\n",
        "  def __init__(self):\n",
        "    self.mdp = GridWorld()\n",
        "    self.state = self.mdp.get_initial_state()\n",
        "    self.possible_actions = self.mdp.get_actions(self.state)\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = self.mdp.get_initial_state()\n",
        "    return self.state \n",
        "    \n",
        "  def step(self, action):\n",
        "    terminal = self.mdp.is_terminal(state)\n",
        "    if not terminal:\n",
        "      next_state, reward = self.mdp.execute(self.state, action)\n",
        "      self.state = next_state\n",
        "      return next_state, reward, terminal\n",
        "\n",
        "    else:\n",
        "      return state, 0, terminal\n"
      ],
      "metadata": {
        "id": "1wSm8qFFFToT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class RandomAgent():\n",
        "  def __init__(self, possible_actions):\n",
        "    self.possible_actions = possible_actions\n",
        "  def step(self, state):\n",
        "    return np.random.choice(self.possible_actions)\n",
        "    "
      ],
      "metadata": {
        "id": "WPut5WsdJvzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "world = UnknownWorld()\n",
        "random_agent = RandomAgent(world.possible_actions)\n",
        "state = world.state\n",
        "terminal = False\n",
        "for step in range(10000):\n",
        "    action = random_agent.step(state)\n",
        "    next_state, reward, terminal = world.step(action)\n",
        "    state = next_state\n",
        "    if terminal:\n",
        "      break\n"
      ],
      "metadata": {
        "id": "0cE2GCQ2JfXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning:\n",
        "  def __init__(self, possible_actions):\n",
        "    self.qfunction = QTable()\n",
        "    self.eps = 0.1\n",
        "    self.possible_actions = possible_actions\n",
        "    self.gamma = 0.99\n",
        "    self.step_size = 0.1\n",
        "\n",
        "  def step(self, state):\n",
        "\n",
        "    if np.random.rand() < self.eps:  \n",
        "      return np.random.choice(self.possible_actions)\n",
        "    else:\n",
        "      (best_action, max_q) = self.qfunction.get_max_q(state, self.possible_actions)\n",
        "      return best_action\n",
        "\n",
        "  def learn_one_step(self, state, action, reward, next_state):\n",
        "    # Q(s,a) = r + gamma * max_b Q(s', b)\n",
        "    current_q = self.qfunction.get_q_value(state, action)\n",
        "    \n",
        "    max_next_state_q = -np.inf\n",
        "    for b in self.possible_actions:\n",
        "      next_state_q = self.qfunction.get_q_value(next_state, b)\n",
        "      if next_state_q > max_next_state_q:\n",
        "        max_next_state_q = next_state_q\n",
        "\n",
        "    new_q     = current_q  + self.step_size * (reward + self.gamma*max_next_state_q - current_q) \n",
        "    self.qfunction.update(state, action, new_q)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Aeucr0NdCoqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "world = UnknownWorld()\n",
        "agent = QLearning(world.possible_actions)\n",
        "\n",
        "for episode in range(2000):\n",
        "  terminal = False\n",
        "  state = world.reset()\n",
        "  for step in range(10000):\n",
        "      \n",
        "    action = agent.step(state)\n",
        "    next_state, reward, terminal = world.step(action)\n",
        "\n",
        "    agent.learn_one_step(state, action, reward, next_state)\n",
        "    state = next_state\n",
        "    if terminal:\n",
        "      break\n"
      ],
      "metadata": {
        "id": "PEZn97yPFRUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gridworld.visualise_q_function(agent.qfunction, \"Q function\")"
      ],
      "metadata": {
        "id": "gIPjBpAsUJNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ts6h92W2UM8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Reinforcement Learning"
      ],
      "metadata": {
        "id": "V1NHFKbqljhY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HfMZze2Fli7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}